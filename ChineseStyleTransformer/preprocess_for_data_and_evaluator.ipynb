{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T07:21:52.380582Z",
     "start_time": "2020-07-22T07:21:52.081177Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## THUCNews\n",
    "\n",
    "Topics: 体育  娱乐  家居  彩票  房产  教育  时尚  时政  星座  游戏  社会  科技  股票  财经\n",
    "\n",
    "### Data Statistics\n",
    "\n",
    "* Get total 23161 articles and 453149 sentences on topic 科技\n",
    "\n",
    "### Garbage Cases\n",
    "\n",
    "#### Author name related\n",
    "\n",
    "```txt\n",
    "英国记者用相机拍摄阅兵\n",
    "　　去年的国庆60周年大阅兵想必大家还记忆犹新，通过电视直播，建国以来几十年的发展变化全都呈现在了观众眼前，据说央视也为了此次直播准备了很久，期间彩排的几次其也是次次到场，而最后的直播画面也被做成集锦，播出了很长时间，但是近期笔者发现了一部由英国记者用数码相机拍摄的国庆阅兵场面。\n",
    "\n",
    "\n",
    "60家网媒编辑记者广东采访 \n",
    "　　本报讯(记者/杨大正)昨日,改革开放30周年全国重点网络媒体广东行在广州拉开序幕。来自人民网、新华网、新浪、网易等60多家全国重点网络媒体的百余名网络编辑、记者将对广州、肇庆、佛山、东莞、深圳等5个城市进行为期一周的采访报道。 \n",
    "```\n",
    "\n",
    "Author name in the last line\n",
    "\n",
    "* usually can be remove by the `remove_too_short` > 4, except some of the cases\n",
    "\n",
    "```txt\n",
    "(任秋凌)(杨孝文)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T07:21:52.385466Z",
     "start_time": "2020-07-22T07:21:52.381774Z"
    }
   },
   "outputs": [],
   "source": [
    "# THUCNews constants\n",
    "\n",
    "topics_to_select = [\n",
    "    {\n",
    "        'label': 'pos',\n",
    "        'topic': '科技'\n",
    "    },\n",
    "    {\n",
    "        'label': 'neg',\n",
    "        'topic': '财经'\n",
    "    }\n",
    "]\n",
    "\n",
    "data_to_split = [\n",
    "    {\n",
    "        'type': 'train',\n",
    "        'amount': None, # the rest\n",
    "    },\n",
    "    {\n",
    "        'type': 'dev',\n",
    "#         'amount': 2000,\n",
    "        'amount': 20,\n",
    "    },\n",
    "    {\n",
    "        'type': 'test',\n",
    "        'amount': 500,\n",
    "#         'amount': 5,\n",
    "    }\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T07:21:52.396503Z",
     "start_time": "2020-07-22T07:21:52.387578Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_word_list(sentence):\n",
    "    \"\"\"\n",
    "    https://blog.csdn.net/weixin_44208569/article/details/90315208\n",
    "    \n",
    "    TODO: this will remove punctuation but we want them\n",
    "    \"\"\"\n",
    "    to_split = re.compile('[\\\\W]*') # string except for what we want\n",
    "    chinese = re.compile(r'([\\u4e00-\\u9fa5])') # Chinese characters\n",
    "\n",
    "    word_list = []\n",
    "    try:\n",
    "        for string in to_split.split(sentence):\n",
    "            if chinese.search(string):\n",
    "                for char in chinese.split(string):\n",
    "                    word_list.append(char)\n",
    "            else:\n",
    "                word_list.append(string)\n",
    "    except:\n",
    "        import ipdb; ipdb.set_trace()\n",
    "\n",
    "    return [word for word in word_list if len(word.strip()) > 0]  # remove empty string\n",
    "\n",
    "\n",
    "\n",
    "def single_news_process(text, drop_empty_line=True, strip=True, remove_author=True, remove_dummy_word_title=True, remove_too_short=5):\n",
    "    \"\"\"\n",
    "    * Seperate sentences\n",
    "    * Seperate paragraph?! (currently we don't do this, if we want to do this maybe we might need to mark the sentence ID of seperation)\n",
    "    \n",
    "    reference: https://github.com/blmoistawinde/HarvestText/blob/73c28ab6549d8a16392fca9803823eaa94221100/harvesttext/harvesttext.py#L711\n",
    "    \"\"\"\n",
    "    \n",
    "    text = re.sub('([。！？\\?!])([^”’])', r\"\\1\\n\\2\", text)\n",
    "    text = re.sub('(\\.{6})([^”’])', r\"\\1\\n\\2\", text)\n",
    "    text = re.sub('(\\…{2})([^”’])', r\"\\1\\n\\2\", text)\n",
    "    text = re.sub('([。！？\\?!][”’])([^，。！？\\?])', r'\\1\\n\\2', text)\n",
    "    text = text.rstrip()\n",
    "    sentences = text.split(\"\\n\")\n",
    "    if strip:\n",
    "        sentences = [sent.strip() for sent in sentences]\n",
    "        \n",
    "    if drop_empty_line:\n",
    "        to_include = max(remove_too_short, 0)\n",
    "        sentences = [sent for sent in sentences if len(sent.strip()) > to_include]\n",
    "        \n",
    "    if remove_author:\n",
    "        author_keywords = ['记者 ', '记者：', '作 者', '作者：', '□ ', '策划/', '策划/']\n",
    "        author_index = min(3, len(sentences)) # only look the first few sentences\n",
    "        i = 1 # usually author information start from second sentence\n",
    "        while i < min(3, len(sentences)):\n",
    "            delete = False\n",
    "            for keyword in author_keywords:\n",
    "                if keyword in sentences[i]:\n",
    "                    del sentences[i]\n",
    "                    delete = True\n",
    "                    break\n",
    "            if not delete:\n",
    "                i += 1\n",
    "        \n",
    "    if remove_dummy_word_title and len(sentences) > 0:\n",
    "        dummy_word = ['(组图)', '(图)']\n",
    "        for keyword in dummy_word:\n",
    "            if sentences[0][-len(keyword):] == keyword:\n",
    "                sentences[0] = sentences[0][:-len(keyword)]\n",
    "                \n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T07:21:52.404823Z",
     "start_time": "2020-07-22T07:21:52.397832Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_topic_of_THUCNews(topic, base_dir = 'data/THUCnews', process_fn = lambda x: x,\n",
    "                          filter_garbage = True, drop_too_short = 10, keep_structure=False, seperate_char=True,\n",
    "                          verbose=False):\n",
    "    all_articles = []\n",
    "    \n",
    "    garbage = re.compile(r'(【ZOL-七天在线|【四川行情】|【3C168 中关村湖南】|【IT168|[参考价格])')\n",
    "    article_count = 0\n",
    "    for article_path in glob(os.path.join(base_dir, topic, '*.txt')):\n",
    "        with open(article_path, 'r') as stream:\n",
    "            article = stream.read()\n",
    "        \n",
    "        if filter_garbage:\n",
    "            if garbage.search(article):\n",
    "                continue\n",
    "        \n",
    "        article_sents = process_fn(article)\n",
    "        if drop_too_short > 0 and drop_too_short > len(article_sents):\n",
    "            continue\n",
    "        \n",
    "        if article_sents:\n",
    "            article_count += 1\n",
    "            \n",
    "            if seperate_char:\n",
    "                # seperate characters\n",
    "                article_sents = [' '.join(sent) for sent in article_sents]\n",
    "                # TODO: combine english but keep punctuation\n",
    "                # article_sents = [' '.join(get_word_list(sentences)) for sent in article_sents]\n",
    "\n",
    "            if keep_structure:\n",
    "                # single article single list\n",
    "                all_articles.append(article_sents)\n",
    "            else:\n",
    "                # all sentences in one list\n",
    "                all_articles.extend(article_sents)\n",
    "                \n",
    "        # debug\n",
    "        if article_count > 57:\n",
    "            break\n",
    "        \n",
    "    if verbose:\n",
    "        if keep_structure:\n",
    "            print('Get total', article_count, 'articles and', sum([len(sents) for sents in all_articles]), 'sentences on topic', topic)\n",
    "        else:\n",
    "            print('Get total', article_count, 'articles and', len(all_articles), 'sentences on topic', topic)\n",
    "\n",
    "    return all_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T07:21:52.413921Z",
     "start_time": "2020-07-22T07:21:52.405997Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get_topic_of_THUCNews('科技', process_fn=single_news_process, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T07:21:52.857070Z",
     "start_time": "2020-07-22T07:21:52.415533Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get total 58 articles and 1384 sentences on topic 科技\n",
      "Get total 58 articles and 1606 sentences on topic 财经\n"
     ]
    }
   ],
   "source": [
    "data = {item['label']: get_topic_of_THUCNews(item['topic'], process_fn=single_news_process, verbose=True) for item in topics_to_select}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T07:21:52.860198Z",
     "start_time": "2020-07-22T07:21:52.858477Z"
    }
   },
   "outputs": [],
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T07:21:52.874970Z",
     "start_time": "2020-07-22T07:21:52.862357Z"
    }
   },
   "outputs": [],
   "source": [
    "# split data\n",
    "import random\n",
    "\n",
    "random.seed(87)\n",
    "splitted_data = {}\n",
    "\n",
    "for label, sents in data.items():\n",
    "    for_the_rest_set = None\n",
    "    total_indices = list(range(len(sents)))\n",
    "    data_temp = {}\n",
    "    for data_set in data_to_split:\n",
    "        data_temp[data_set['type']] = []\n",
    "        if data_set['amount'] is None:\n",
    "            for_the_rest_set = data_set['type']\n",
    "        else:\n",
    "            sample_indices = random.sample(total_indices, data_set['amount'])\n",
    "            for index in sample_indices:\n",
    "                data_temp[data_set['type']].append(sents[index])\n",
    "                total_indices.remove(index)\n",
    "                \n",
    "    if for_the_rest_set:\n",
    "        for index in total_indices:\n",
    "            data_temp[for_the_rest_set].append(sents[index])\n",
    "    \n",
    "    splitted_data[label] = data_temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T07:21:52.885101Z",
     "start_time": "2020-07-22T07:21:52.876607Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos\n",
      "train 864\n",
      "dev 20\n",
      "test 500\n",
      "neg\n",
      "train 1086\n",
      "dev 20\n",
      "test 500\n"
     ]
    }
   ],
   "source": [
    "for label, sub_data in splitted_data.items():\n",
    "    print(label)\n",
    "    for data_set, sents in sub_data.items():\n",
    "        print(data_set, len(sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T07:21:52.936727Z",
     "start_time": "2020-07-22T07:21:52.886243Z"
    }
   },
   "outputs": [],
   "source": [
    "# Write data\n",
    "\n",
    "for label, sub_data in splitted_data.items():\n",
    "    for data_set, sents in sub_data.items():\n",
    "        with open(os.path.join('data/THUCNews', f'{data_set}.{label}'), 'w') as stream:\n",
    "            for sent in sents:\n",
    "                stream.write(sent + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T07:23:55.462212Z",
     "start_time": "2020-07-22T07:23:55.142148Z"
    }
   },
   "outputs": [],
   "source": [
    "!cp 'data/THUCNews/test.pos' 'evaluator/THUCNews.refs.1'\n",
    "!cp 'data/THUCNews/test.neg' 'evaluator/THUCNews.refs.0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Evaluator\n",
    "\n",
    "* [Preperation for Evaluator for New Dataset - HackMD](https://hackmd.io/NgYXPtOqRCWKHV33L1NofQ?view)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fasttext classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T07:21:52.947896Z",
     "start_time": "2020-07-22T07:21:52.938013Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate training files\n",
    "\n",
    "with open('data/THUCNews_data_train.txt', 'w') as stream:\n",
    "    for label, sub_data in splitted_data.items():\n",
    "        for sent in sub_data['train']:\n",
    "            stream.write(f'__label__{label} {sent}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T07:21:53.155095Z",
     "start_time": "2020-07-22T07:21:52.949292Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train classifier\n",
    "\n",
    "import fasttext\n",
    "\n",
    "\n",
    "model = fasttext.train_supervised('data/THUCNews_data_train.txt')\n",
    "model.save_model('evaluator/acc_THUCNews.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kenlm for perplexity evaluation\n",
    "\n",
    "> (make sure you have execute `setup.sh` before to get the kenlm executable)\n",
    "\n",
    "related links:\n",
    "\n",
    "* [使用KenLM训练n-gram语言模型 （中文）_benbenls的博客-CSDN博客_kenlm 中文](https://blog.csdn.net/benbenls/article/details/102898960)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T07:21:53.165942Z",
     "start_time": "2020-07-22T07:21:53.156207Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('data/THUCNews_lm_data.txt', 'w') as stream:\n",
    "    for label, sub_data in splitted_data.items():\n",
    "        for sent in sub_data['train']:\n",
    "            stream.write(sent + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T07:21:55.063162Z",
     "start_time": "2020-07-22T07:21:53.167021Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /mnt/d/Program/TextStyleTransfer/ChineseStyleTransformer/data/THUCNews_lm_data.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 79746 types 2230\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:26760 2:989202560 3:1854754944 4:2967607552 5:4327761408\n",
      "Statistics:\n",
      "1 2230 D1=0.50566 D2=0.899899 D3+=1.61609\n",
      "2 31155 D1=0.756436 D2=1.14743 D3+=1.44966\n",
      "3 56421 D1=0.867392 D2=1.31004 D3+=1.66164\n",
      "4 66124 D1=0.935267 D2=1.41032 D3+=1.75298\n",
      "5 69220 D1=0.901285 D2=1.62734 D3+=1.91625\n",
      "Memory estimate for binary LM:\n",
      "type      kB\n",
      "probing 4875 assuming -p 1.5\n",
      "probing 5785 assuming -r models -p 1.5\n",
      "trie    2138 without quantization\n",
      "trie    1068 assuming -q 8 -b 8 quantization \n",
      "trie    1958 assuming -a 22 array pointer compression\n",
      "trie     889 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:26760 2:498480 3:1128420 4:1586976 5:1938160\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:26760 2:498480 3:1128420 4:1586976 5:1938160\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Name:lmplz\tVmPeak:0 kB\tVmRSS:3796 kB\tRSSMax:1582616 kB\tuser:0.40625\tsys:1.17188\tCPU:1.57812\treal:1.74472\n"
     ]
    }
   ],
   "source": [
    "!kenlm/build/bin/lmplz -o 5 <data/THUCNews_lm_data.txt >data/THUCNews.arpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T07:21:55.315430Z",
     "start_time": "2020-07-22T07:21:55.064596Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data/THUCNews.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!kenlm/build/bin/build_binary data/THUCNews.arpa evaluator/ppl_THUCNews.binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
