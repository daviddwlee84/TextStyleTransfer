{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T09:29:18.060717Z",
     "start_time": "2020-07-22T09:29:17.586001Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## THUCNews\n",
    "\n",
    "Topics: 体育  娱乐  家居  彩票  房产  教育  时尚  时政  星座  游戏  社会  科技  股票  财经\n",
    "\n",
    "### Data Statistics\n",
    "\n",
    "* Get total 23161 articles and 453149 sentences on topic 科技\n",
    "\n",
    "### Garbage Cases\n",
    "\n",
    "#### Author name related\n",
    "\n",
    "```txt\n",
    "英国记者用相机拍摄阅兵\n",
    "　　去年的国庆60周年大阅兵想必大家还记忆犹新，通过电视直播，建国以来几十年的发展变化全都呈现在了观众眼前，据说央视也为了此次直播准备了很久，期间彩排的几次其也是次次到场，而最后的直播画面也被做成集锦，播出了很长时间，但是近期笔者发现了一部由英国记者用数码相机拍摄的国庆阅兵场面。\n",
    "\n",
    "\n",
    "60家网媒编辑记者广东采访 \n",
    "　　本报讯(记者/杨大正)昨日,改革开放30周年全国重点网络媒体广东行在广州拉开序幕。来自人民网、新华网、新浪、网易等60多家全国重点网络媒体的百余名网络编辑、记者将对广州、肇庆、佛山、东莞、深圳等5个城市进行为期一周的采访报道。 \n",
    "```\n",
    "\n",
    "Author name in the last line\n",
    "\n",
    "* usually can be remove by the `remove_too_short` > 4, except some of the cases\n",
    "\n",
    "```txt\n",
    "(任秋凌)(杨孝文)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T09:29:18.064681Z",
     "start_time": "2020-07-22T09:29:18.061936Z"
    }
   },
   "outputs": [],
   "source": [
    "# THUCNews constants\n",
    "\n",
    "topics_to_select = [\n",
    "    {\n",
    "        'label': 'pos',\n",
    "        'topic': '科技'\n",
    "    },\n",
    "    {\n",
    "        'label': 'neg',\n",
    "        'topic': '财经'\n",
    "    }\n",
    "]\n",
    "\n",
    "data_to_split = [\n",
    "    {\n",
    "        'type': 'train',\n",
    "        'amount': None, # the rest\n",
    "    },\n",
    "    {\n",
    "        'type': 'dev',\n",
    "        'amount': 2000,\n",
    "#         'amount': 20, # debug\n",
    "    },\n",
    "    {\n",
    "        'type': 'test',\n",
    "        'amount': 500,\n",
    "#         'amount': 5, # debug\n",
    "    }\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T09:29:18.077399Z",
     "start_time": "2020-07-22T09:29:18.066759Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_word_list(sentence):\n",
    "    \"\"\"\n",
    "    https://blog.csdn.net/weixin_44208569/article/details/90315208\n",
    "    \n",
    "    TODO: this will remove punctuation but we want them\n",
    "    \"\"\"\n",
    "    to_split = re.compile('[\\\\W]*') # string except for what we want\n",
    "    chinese = re.compile(r'([\\u4e00-\\u9fa5])') # Chinese characters\n",
    "\n",
    "    word_list = []\n",
    "    # TODO: need some debug here\n",
    "    try:\n",
    "        for string in to_split.split(sentence):\n",
    "            if chinese.search(string):\n",
    "                for char in chinese.split(string):\n",
    "                    word_list.append(char)\n",
    "            else:\n",
    "                word_list.append(string)\n",
    "    except:\n",
    "        import ipdb; ipdb.set_trace()\n",
    "\n",
    "    return [word for word in word_list if len(word.strip()) > 0]  # remove empty string\n",
    "\n",
    "\n",
    "\n",
    "def single_news_process(text, drop_empty_line=True, strip=True, remove_author=True, remove_dummy_word_title=True, remove_too_short=5):\n",
    "    \"\"\"\n",
    "    * Seperate sentences\n",
    "    * Seperate paragraph?! (currently we don't do this, if we want to do this maybe we might need to mark the sentence ID of seperation)\n",
    "    \n",
    "    reference: https://github.com/blmoistawinde/HarvestText/blob/73c28ab6549d8a16392fca9803823eaa94221100/harvesttext/harvesttext.py#L711\n",
    "    \"\"\"\n",
    "    \n",
    "    text = re.sub('([。！？\\?!])([^”’])', r\"\\1\\n\\2\", text)\n",
    "    text = re.sub('(\\.{6})([^”’])', r\"\\1\\n\\2\", text)\n",
    "    text = re.sub('(\\…{2})([^”’])', r\"\\1\\n\\2\", text)\n",
    "    text = re.sub('([。！？\\?!][”’])([^，。！？\\?])', r'\\1\\n\\2', text)\n",
    "    text = text.rstrip()\n",
    "    sentences = text.split(\"\\n\")\n",
    "    if strip:\n",
    "        sentences = [sent.strip() for sent in sentences]\n",
    "        \n",
    "    if drop_empty_line:\n",
    "        to_include = max(remove_too_short, 0)\n",
    "        sentences = [sent for sent in sentences if len(sent.strip()) > to_include]\n",
    "        \n",
    "    if remove_author:\n",
    "        author_keywords = ['记者 ', '记者：', '作 者', '作者：', '□ ', '策划/', '策划/']\n",
    "        author_index = min(3, len(sentences)) # only look the first few sentences\n",
    "        i = 1 # usually author information start from second sentence\n",
    "        while i < min(3, len(sentences)):\n",
    "            delete = False\n",
    "            for keyword in author_keywords:\n",
    "                if keyword in sentences[i]:\n",
    "                    del sentences[i]\n",
    "                    delete = True\n",
    "                    break\n",
    "            if not delete:\n",
    "                i += 1\n",
    "        \n",
    "    if remove_dummy_word_title and len(sentences) > 0:\n",
    "        dummy_word = ['(组图)', '(图)']\n",
    "        for keyword in dummy_word:\n",
    "            if sentences[0][-len(keyword):] == keyword:\n",
    "                sentences[0] = sentences[0][:-len(keyword)]\n",
    "                \n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T09:29:18.086754Z",
     "start_time": "2020-07-22T09:29:18.078848Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_topic_of_THUCNews(topic, base_dir = 'data/THUCNews', process_fn = lambda x: x,\n",
    "                          filter_garbage = True, drop_too_short = 10, keep_structure=False, seperate_char=True,\n",
    "                          verbose=False):\n",
    "    all_articles = []\n",
    "    \n",
    "    garbage = re.compile(r'(【ZOL-七天在线|【四川行情】|【3C168 中关村湖南】|【IT168|[参考价格])')\n",
    "    article_count = 0\n",
    "    for article_path in glob(os.path.join(base_dir, topic, '*.txt')):\n",
    "        with open(article_path, 'r') as stream:\n",
    "            article = stream.read()\n",
    "        \n",
    "        if filter_garbage:\n",
    "            if garbage.search(article):\n",
    "                continue\n",
    "        \n",
    "        article_sents = process_fn(article)\n",
    "        if drop_too_short > 0 and drop_too_short > len(article_sents):\n",
    "            continue\n",
    "        \n",
    "        if article_sents:\n",
    "            article_count += 1\n",
    "            \n",
    "            if seperate_char:\n",
    "                # seperate characters\n",
    "                article_sents = [' '.join(sent) for sent in article_sents]\n",
    "                # TODO: combine english but keep punctuation\n",
    "                # article_sents = [' '.join(get_word_list(sentences)) for sent in article_sents]\n",
    "\n",
    "            if keep_structure:\n",
    "                # single article single list\n",
    "                all_articles.append(article_sents)\n",
    "            else:\n",
    "                # all sentences in one list\n",
    "                all_articles.extend(article_sents)\n",
    "                \n",
    "        # debug\n",
    "        # if article_count > 57:\n",
    "        #     break\n",
    "        \n",
    "    if verbose:\n",
    "        if keep_structure:\n",
    "            print('Get total', article_count, 'articles and', sum([len(sents) for sents in all_articles]), 'sentences on topic', topic)\n",
    "        else:\n",
    "            print('Get total', article_count, 'articles and', len(all_articles), 'sentences on topic', topic)\n",
    "\n",
    "    return all_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T09:29:18.096341Z",
     "start_time": "2020-07-22T09:29:18.088196Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get_topic_of_THUCNews('科技', process_fn=single_news_process, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T09:29:19.067909Z",
     "start_time": "2020-07-22T09:29:18.097605Z"
    }
   },
   "outputs": [],
   "source": [
    "data = {item['label']: get_topic_of_THUCNews(item['topic'], process_fn=single_news_process, verbose=True) for item in topics_to_select}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T09:29:19.071332Z",
     "start_time": "2020-07-22T09:29:19.069105Z"
    }
   },
   "outputs": [],
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T09:29:32.771752Z",
     "start_time": "2020-07-22T09:29:32.767574Z"
    }
   },
   "outputs": [],
   "source": [
    "# split data\n",
    "# maybe shuffle the data?!\n",
    "# import random\n",
    "# random.seed(87)\n",
    "\n",
    "splitted_data = {}\n",
    "\n",
    "for label, sents in data.items():\n",
    "    for_the_rest_set = None\n",
    "    total_indices = list(range(len(sents)))\n",
    "    data_temp = {}\n",
    "    start_index = 0\n",
    "    for data_set in data_to_split:\n",
    "        data_temp[data_set['type']] = []\n",
    "        if data_set['amount'] is None:\n",
    "            for_the_rest_set = data_set['type']\n",
    "        else:\n",
    "            # sample_indices = random.sample(total_indices, data_set['amount'])\n",
    "            # for index in sample_indices:\n",
    "            #     data_temp[data_set['type']].append(sents[index])\n",
    "            #     total_indices.remove(index)\n",
    "            data_temp[data_set['type']] = sents[start_index:start_index + data_set['amount']]\n",
    "            start_index = data_set['amount']\n",
    "                \n",
    "    if for_the_rest_set:\n",
    "        # for index in total_indices:\n",
    "        #     data_temp[for_the_rest_set].append(sents[index])\n",
    "        data_temp[for_the_rest_set] = sents[start_index:]\n",
    "    \n",
    "    splitted_data[label] = data_temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T09:29:33.799724Z",
     "start_time": "2020-07-22T09:29:33.796212Z"
    }
   },
   "outputs": [],
   "source": [
    "for label, sub_data in splitted_data.items():\n",
    "    print(label)\n",
    "    for data_set, sents in sub_data.items():\n",
    "        print(data_set, len(sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T09:29:34.011749Z",
     "start_time": "2020-07-22T09:29:33.966887Z"
    }
   },
   "outputs": [],
   "source": [
    "# Write data\n",
    "\n",
    "for label, sub_data in splitted_data.items():\n",
    "    for data_set, sents in sub_data.items():\n",
    "        with open(os.path.join('data/THUCNews', f'{data_set}.{label}'), 'w') as stream:\n",
    "            for sent in sents:\n",
    "                stream.write(sent + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T09:29:34.423649Z",
     "start_time": "2020-07-22T09:29:34.126490Z"
    }
   },
   "outputs": [],
   "source": [
    "!cp 'data/THUCNews/test.pos' 'evaluator/THUCNews.refs.1'\n",
    "!cp 'data/THUCNews/test.neg' 'evaluator/THUCNews.refs.0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Evaluator\n",
    "\n",
    "* [Preperation for Evaluator for New Dataset - HackMD](https://hackmd.io/NgYXPtOqRCWKHV33L1NofQ?view)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fasttext classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T09:29:36.743683Z",
     "start_time": "2020-07-22T09:29:36.729807Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate training files\n",
    "\n",
    "with open('data/THUCNews_data_train.txt', 'w') as stream:\n",
    "    for label, sub_data in splitted_data.items():\n",
    "        for sent in sub_data['train']:\n",
    "            stream.write(f'__label__{label} {sent}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T09:29:19.164458Z",
     "start_time": "2020-07-22T09:29:17.605Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train classifier\n",
    "\n",
    "import fasttext\n",
    "\n",
    "\n",
    "model = fasttext.train_supervised('data/THUCNews_data_train.txt')\n",
    "model.save_model('evaluator/acc_THUCNews.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kenlm for perplexity evaluation\n",
    "\n",
    "> (make sure you have execute `setup.sh` before to get the kenlm executable)\n",
    "\n",
    "related links:\n",
    "\n",
    "* [使用KenLM训练n-gram语言模型 （中文）_benbenls的博客-CSDN博客_kenlm 中文](https://blog.csdn.net/benbenls/article/details/102898960)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T09:29:19.165623Z",
     "start_time": "2020-07-22T09:29:17.607Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('data/THUCNews_lm_data.txt', 'w') as stream:\n",
    "    for label, sub_data in splitted_data.items():\n",
    "        for sent in sub_data['train']:\n",
    "            stream.write(sent + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T09:29:19.166524Z",
     "start_time": "2020-07-22T09:29:17.609Z"
    }
   },
   "outputs": [],
   "source": [
    "!kenlm/build/bin/lmplz -o 5 <data/THUCNews_lm_data.txt >data/THUCNews.arpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T09:29:19.167396Z",
     "start_time": "2020-07-22T09:29:17.613Z"
    }
   },
   "outputs": [],
   "source": [
    "!kenlm/build/bin/build_binary data/THUCNews.arpa evaluator/ppl_THUCNews.binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
