{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T09:29:18.060717Z",
     "start_time": "2020-07-22T09:29:17.586001Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## THUCNews\n",
    "\n",
    "Topics: 体育  娱乐  家居  彩票  房产  教育  时尚  时政  星座  游戏  社会  科技  股票  财经\n",
    "\n",
    "### Data Statistics\n",
    "\n",
    "* Get total 23161 articles and 453149 sentences on topic 科技\n",
    "\n",
    "### Garbage Cases\n",
    "\n",
    "#### Author name related\n",
    "\n",
    "```txt\n",
    "英国记者用相机拍摄阅兵\n",
    "　　去年的国庆60周年大阅兵想必大家还记忆犹新，通过电视直播，建国以来几十年的发展变化全都呈现在了观众眼前，据说央视也为了此次直播准备了很久，期间彩排的几次其也是次次到场，而最后的直播画面也被做成集锦，播出了很长时间，但是近期笔者发现了一部由英国记者用数码相机拍摄的国庆阅兵场面。\n",
    "\n",
    "\n",
    "60家网媒编辑记者广东采访 \n",
    "　　本报讯(记者/杨大正)昨日,改革开放30周年全国重点网络媒体广东行在广州拉开序幕。来自人民网、新华网、新浪、网易等60多家全国重点网络媒体的百余名网络编辑、记者将对广州、肇庆、佛山、东莞、深圳等5个城市进行为期一周的采访报道。 \n",
    "```\n",
    "\n",
    "Author name in the last line\n",
    "\n",
    "* usually can be remove by the `remove_too_short` > 4, except some of the cases\n",
    "\n",
    "```txt\n",
    "(任秋凌)(杨孝文)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "科技 162929\n",
      "时政 63086\n",
      "家居 32586\n",
      "社会 50849\n",
      "股票 154398\n",
      "时尚 13368\n",
      "彩票 7588\n",
      "娱乐 92632\n",
      "财经 37098\n",
      "教育 41936\n",
      "游戏 24373\n",
      "星座 3578\n",
      "房产 20050\n",
      "体育 131604\n"
     ]
    }
   ],
   "source": [
    "# data count\n",
    "for item in os.listdir('data/THUCNews'):\n",
    "    if os.path.isdir(os.path.join('data/THUCNews', item)):\n",
    "        print(item, len(os.listdir(os.path.join('data/THUCNews', item))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T09:29:18.064681Z",
     "start_time": "2020-07-22T09:29:18.061936Z"
    }
   },
   "outputs": [],
   "source": [
    "# THUCNews constants\n",
    "\n",
    "topics_to_select = [\n",
    "    {\n",
    "        'label': 'pos',\n",
    "        'topic': '科技'\n",
    "    },\n",
    "    {\n",
    "        'label': 'neg',\n",
    "        'topic': '体育'\n",
    "    }\n",
    "]\n",
    "\n",
    "data_to_split = [\n",
    "    {\n",
    "        'type': 'train',\n",
    "        'amount': None, # the rest\n",
    "    },\n",
    "    {\n",
    "        'type': 'dev',\n",
    "        'amount': 2000,\n",
    "#         'amount': 20, # debug\n",
    "    },\n",
    "    {\n",
    "        'type': 'test',\n",
    "        'amount': 500,\n",
    "#         'amount': 5, # debug\n",
    "    }\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T09:29:18.077399Z",
     "start_time": "2020-07-22T09:29:18.066759Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_word_list(sentence):\n",
    "    \"\"\"\n",
    "    https://blog.csdn.net/weixin_44208569/article/details/90315208\n",
    "    \n",
    "    TODO: this will remove punctuation but we want them\n",
    "    \"\"\"\n",
    "    to_split = re.compile('[\\\\W]*') # string except for what we want\n",
    "    chinese = re.compile(r'([\\u4e00-\\u9fa5])') # Chinese characters\n",
    "\n",
    "    word_list = []\n",
    "    # TODO: need some debug here\n",
    "    try:\n",
    "        for string in to_split.split(sentence):\n",
    "            if chinese.search(string):\n",
    "                for char in chinese.split(string):\n",
    "                    word_list.append(char)\n",
    "            else:\n",
    "                word_list.append(string)\n",
    "    except:\n",
    "        import ipdb; ipdb.set_trace()\n",
    "\n",
    "    return [word for word in word_list if len(word.strip()) > 0]  # remove empty string\n",
    "\n",
    "\n",
    "\n",
    "def single_news_process(text, drop_empty_line=True, strip=True, remove_author=True, remove_dummy_word_title=True, remove_too_short=5):\n",
    "    \"\"\"\n",
    "    * Seperate sentences\n",
    "    * Seperate paragraph?! (currently we don't do this, if we want to do this maybe we might need to mark the sentence ID of seperation)\n",
    "    \n",
    "    reference: https://github.com/blmoistawinde/HarvestText/blob/73c28ab6549d8a16392fca9803823eaa94221100/harvesttext/harvesttext.py#L711\n",
    "    \"\"\"\n",
    "    \n",
    "    text = re.sub('([。！？\\?!])([^”’])', r\"\\1\\n\\2\", text)\n",
    "    text = re.sub('(\\.{6})([^”’])', r\"\\1\\n\\2\", text)\n",
    "    text = re.sub('(\\…{2})([^”’])', r\"\\1\\n\\2\", text)\n",
    "    text = re.sub('([。！？\\?!][”’])([^，。！？\\?])', r'\\1\\n\\2', text)\n",
    "    text = text.rstrip()\n",
    "    sentences = text.split(\"\\n\")\n",
    "    if strip:\n",
    "        sentences = [sent.strip() for sent in sentences]\n",
    "        \n",
    "    if drop_empty_line:\n",
    "        to_include = max(remove_too_short, 0)\n",
    "        sentences = [sent for sent in sentences if len(sent.strip()) > to_include]\n",
    "        \n",
    "    if remove_author:\n",
    "        author_keywords = ['记者 ', '记者：', '作 者', '作者：', '□ ', '策划/', '策划/']\n",
    "        author_index = min(3, len(sentences)) # only look the first few sentences\n",
    "        i = 1 # usually author information start from second sentence\n",
    "        while i < min(3, len(sentences)):\n",
    "            delete = False\n",
    "            for keyword in author_keywords:\n",
    "                if keyword in sentences[i]:\n",
    "                    del sentences[i]\n",
    "                    delete = True\n",
    "                    break\n",
    "            if not delete:\n",
    "                i += 1\n",
    "        \n",
    "    if remove_dummy_word_title and len(sentences) > 0:\n",
    "        dummy_word = ['(组图)', '(图)']\n",
    "        for keyword in dummy_word:\n",
    "            if sentences[0][-len(keyword):] == keyword:\n",
    "                sentences[0] = sentences[0][:-len(keyword)]\n",
    "                \n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T09:29:18.086754Z",
     "start_time": "2020-07-22T09:29:18.078848Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_topic_of_THUCNews(topic, base_dir = 'data/THUCNews', process_fn = lambda x: x,\n",
    "                          filter_garbage = True, drop_too_short = 10, keep_structure=False, seperate_char=True,\n",
    "                          verbose=False):\n",
    "    all_articles = []\n",
    "    \n",
    "    garbage = re.compile(r'(【ZOL-七天在线|【四川行情】|【3C168 中关村湖南】|【IT168|[参考价格])')\n",
    "    article_count = 0\n",
    "    for article_path in glob(os.path.join(base_dir, topic, '*.txt')):\n",
    "        with open(article_path, 'r') as stream:\n",
    "            article = stream.read()\n",
    "        \n",
    "        if filter_garbage:\n",
    "            if garbage.search(article):\n",
    "                continue\n",
    "        \n",
    "        article_sents = process_fn(article)\n",
    "        if drop_too_short > 0 and drop_too_short > len(article_sents):\n",
    "            continue\n",
    "        \n",
    "        if article_sents:\n",
    "            article_count += 1\n",
    "            \n",
    "            if seperate_char:\n",
    "                # seperate characters\n",
    "                article_sents = [' '.join(sent) for sent in article_sents]\n",
    "                # TODO: combine english but keep punctuation\n",
    "                # article_sents = [' '.join(get_word_list(sentences)) for sent in article_sents]\n",
    "\n",
    "            if keep_structure:\n",
    "                # single article single list\n",
    "                all_articles.append(article_sents)\n",
    "            else:\n",
    "                # all sentences in one list\n",
    "                all_articles.extend(article_sents)\n",
    "                \n",
    "        # debug\n",
    "        # if article_count > 57:\n",
    "        #     break\n",
    "        \n",
    "    if verbose:\n",
    "        if keep_structure:\n",
    "            print('Get total', article_count, 'articles and', sum([len(sents) for sents in all_articles]), 'sentences on topic', topic)\n",
    "        else:\n",
    "            print('Get total', article_count, 'articles and', len(all_articles), 'sentences on topic', topic)\n",
    "\n",
    "    return all_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T09:29:18.096341Z",
     "start_time": "2020-07-22T09:29:18.088196Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get_topic_of_THUCNews('科技', process_fn=single_news_process, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T09:29:19.067909Z",
     "start_time": "2020-07-22T09:29:18.097605Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get total 21953 articles and 423614 sentences on topic 科技\n",
      "Get total 31960 articles and 708573 sentences on topic 体育\n"
     ]
    }
   ],
   "source": [
    "data = {item['label']: get_topic_of_THUCNews(item['topic'], process_fn=single_news_process, verbose=True) for item in topics_to_select}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T09:29:19.071332Z",
     "start_time": "2020-07-22T09:29:19.069105Z"
    }
   },
   "outputs": [],
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T09:29:32.771752Z",
     "start_time": "2020-07-22T09:29:32.767574Z"
    }
   },
   "outputs": [],
   "source": [
    "# split data\n",
    "# maybe shuffle the data?!\n",
    "# import random\n",
    "# random.seed(87)\n",
    "\n",
    "splitted_data = {}\n",
    "\n",
    "for label, sents in data.items():\n",
    "    for_the_rest_set = None\n",
    "    total_indices = list(range(len(sents)))\n",
    "    data_temp = {}\n",
    "    start_index = 0\n",
    "    for data_set in data_to_split:\n",
    "        data_temp[data_set['type']] = []\n",
    "        if data_set['amount'] is None:\n",
    "            for_the_rest_set = data_set['type']\n",
    "        else:\n",
    "            # sample_indices = random.sample(total_indices, data_set['amount'])\n",
    "            # for index in sample_indices:\n",
    "            #     data_temp[data_set['type']].append(sents[index])\n",
    "            #     total_indices.remove(index)\n",
    "            data_temp[data_set['type']] = sents[start_index:start_index + data_set['amount']]\n",
    "            start_index = data_set['amount']\n",
    "                \n",
    "    if for_the_rest_set:\n",
    "        # for index in total_indices:\n",
    "        #     data_temp[for_the_rest_set].append(sents[index])\n",
    "        data_temp[for_the_rest_set] = sents[start_index:]\n",
    "    \n",
    "    splitted_data[label] = data_temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T09:29:33.799724Z",
     "start_time": "2020-07-22T09:29:33.796212Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos\n",
      "train 423114\n",
      "dev 2000\n",
      "test 500\n",
      "neg\n",
      "train 708073\n",
      "dev 2000\n",
      "test 500\n"
     ]
    }
   ],
   "source": [
    "for label, sub_data in splitted_data.items():\n",
    "    print(label)\n",
    "    for data_set, sents in sub_data.items():\n",
    "        print(data_set, len(sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T09:29:34.011749Z",
     "start_time": "2020-07-22T09:29:33.966887Z"
    }
   },
   "outputs": [],
   "source": [
    "# Write data\n",
    "\n",
    "for label, sub_data in splitted_data.items():\n",
    "    for data_set, sents in sub_data.items():\n",
    "        with open(os.path.join('data/THUCNews', f'{data_set}.{label}'), 'w') as stream:\n",
    "            for sent in sents:\n",
    "                stream.write(sent + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T09:29:34.423649Z",
     "start_time": "2020-07-22T09:29:34.126490Z"
    }
   },
   "outputs": [],
   "source": [
    "!cp 'data/THUCNews/test.pos' 'evaluator/THUCNews.refs.1'\n",
    "!cp 'data/THUCNews/test.neg' 'evaluator/THUCNews.refs.0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Evaluator\n",
    "\n",
    "* [Preperation for Evaluator for New Dataset - HackMD](https://hackmd.io/NgYXPtOqRCWKHV33L1NofQ?view)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fasttext classifier\n",
    "\n",
    "* [How does FastText classifier work under the hood? | by Amjad Abu-Rmileh | Towards Data Science](https://towardsdatascience.com/fasttext-bag-of-tricks-for-efficient-text-classification-513ba9e302e7#:~:text=FastText%2C%20by%20Facebook%20Research%2C%20is,representations%20of%20words%20and%20sentences.)\n",
    "* [Text classification · fastText](https://fasttext.cc/docs/en/supervised-tutorial.html)\n",
    "* [Cheatsheet · fastText](https://fasttext.cc/docs/en/cheatsheet.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T09:29:36.743683Z",
     "start_time": "2020-07-22T09:29:36.729807Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate training files\n",
    "\n",
    "with open('data/THUCNews_data_train.txt', 'w') as stream:\n",
    "    for label, sub_data in splitted_data.items():\n",
    "        for sent in sub_data['train']:\n",
    "            stream.write(f'__label__{label} {sent}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T09:29:19.164458Z",
     "start_time": "2020-07-22T09:29:17.605Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train classifier\n",
    "\n",
    "import fasttext\n",
    "\n",
    "\n",
    "model = fasttext.train_supervised('data/THUCNews_data_train.txt')\n",
    "model.save_model('evaluator/acc_THUCNews.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 0.9415, 0.9415)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quick evaluation\n",
    "\n",
    "with open('data/THUCNews_data_valid.txt', 'w') as stream:\n",
    "    for label, sub_data in splitted_data.items():\n",
    "        for sent in sub_data['dev']:\n",
    "            stream.write(f'__label__{label} {sent}\\n')\n",
    "\n",
    "#(#sample, precision at one, recall at one)\n",
    "model.test('data/THUCNews_data_valid.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kenlm for perplexity evaluation\n",
    "\n",
    "> (make sure you have execute `setup.sh` before to get the kenlm executable)\n",
    "\n",
    "related links:\n",
    "\n",
    "* [使用KenLM训练n-gram语言模型 （中文）_benbenls的博客-CSDN博客_kenlm 中文](https://blog.csdn.net/benbenls/article/details/102898960)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T09:29:19.165623Z",
     "start_time": "2020-07-22T09:29:17.607Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('data/THUCNews_lm_data.txt', 'w') as stream:\n",
    "    for label, sub_data in splitted_data.items():\n",
    "        for sent in sub_data['train']:\n",
    "            stream.write(sent + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T09:29:19.166524Z",
     "start_time": "2020-07-22T09:29:17.609Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /tf/t-dawli/TextStyleTransfer/ChineseStyleTransformer/data/THUCNews_lm_data.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 47797171 types 5683\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:68196 2:21103661056 3:39569367040 4:63310983168 5:92328517632\n",
      "Statistics:\n",
      "1 5683 D1=0.468584 D2=1.195 D3+=1.47069\n",
      "2 1230755 D1=0.614144 D2=1.07658 D3+=1.52494\n",
      "3 8110204 D1=0.737731 D2=1.11805 D3+=1.42726\n",
      "4 19161533 D1=0.827161 D2=1.16742 D3+=1.4093\n",
      "5 28392852 D1=0.790367 D2=1.35546 D3+=1.5094\n",
      "Memory estimate for binary LM:\n",
      "type      MB\n",
      "probing 1139 assuming -p 1.5\n",
      "probing 1303 assuming -r models -p 1.5\n",
      "trie     491 without quantization\n",
      "trie     254 assuming -q 8 -b 8 quantization \n",
      "trie     433 assuming -a 22 array pointer compression\n",
      "trie     196 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:68196 2:19692080 3:162204080 4:459876792 5:794999856\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:68196 2:19692080 3:162204080 4:459876792 5:794999856\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Name:lmplz\tVmPeak:211441632 kB\tVmRSS:25908 kB\tRSSMax:38089864 kB\tuser:61.1317\tsys:29.0223\tCPU:90.1541\treal:92.0302\n"
     ]
    }
   ],
   "source": [
    "!kenlm/build/bin/lmplz -o 5 <data/THUCNews_lm_data.txt >data/THUCNews.arpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T09:29:19.167396Z",
     "start_time": "2020-07-22T09:29:17.613Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data/THUCNews.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!kenlm/build/bin/build_binary data/THUCNews.arpa evaluator/ppl_THUCNews.binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
